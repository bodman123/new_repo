# The validation set approach (Figure 5.2) -----
library(tidyverse)

Auto <- tibble(ISLR::Auto)

# Marginal p-values (as we did in Chapter 3):
summary(lm(mpg ~ horsepower, data = Auto))
summary(lm(mpg ~ poly(horsepower, 2, raw = TRUE), data = Auto))
summary(lm(mpg ~ poly(horsepower, 3, raw = TRUE), data = Auto))


# Validation set:
n <- nrow(Auto)
# n_train <- n / 2
# n_test <- n / 2  # should be n_validation for consistency with lecture notes. 
n_train <- floor(8 * n / 10)
n_test <- n - n_train

M <- 10
poly_degrees <- 1:10

set.seed(1) # this statement fixes the origin of the pseudo-random sequence of numbers
# that are generated by the computer. 
MSEP_df <- tibble(m = rep(NA, M * length(poly_degrees)), deg = NA, MSEP = NA)
j <- 1
for (m in 1:M) {
  Auto$set <- "Train"
  # set.seed(1) # what would happen?
  Auto$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
  df_train <- Auto %>% filter(set == "Train")
  df_test <- Auto %>% filter(set == "Test")

  for (deg in poly_degrees) {
    fit <- lm(mpg ~ poly(horsepower, deg, raw = TRUE), data = df_train)
    pred <- predict(fit, newdata = df_test)
    
    MSEP_df$MSEP[j] <- mean( (df_test$mpg - pred) ^ 2 )
    MSEP_df$m[j] <- m
    MSEP_df$deg[j] <- deg
    
    j <- j + 1
  }    
}
View(MSEP_df)

g0 <-
ggplot(MSEP_df %>% filter(m == 1), aes(x = deg, y = MSEP)) +
  geom_line(color = "red") +
  geom_point(color = "red") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  xlab("Degree of Polynomial") +
  scale_y_continuous(breaks = seq(16, 28, 2), limits = c(16,28))
  
g <-
ggplot(MSEP_df) +
  geom_line(aes(x = deg, y = MSEP, color = factor(m))) +
  theme_bw() +
  # theme(legend.position = "none") +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  xlab("Degree of Polynomial") +
  scale_y_continuous(breaks = seq(16, 28, 2), limits = c(16,28))

library(plotly)
ggplotly(g)

# LOOCV (Figure 5.4, left) -----

Auto <- tibble(ISLR::Auto)

# Brute force approach (fitting the model each time):
{
  t00 <- Sys.time()
  
  n <- nrow(Auto)
  poly_degrees <- 1:10
  MSEP_df <- tibble(i = rep(NA, n * length(poly_degrees)), deg = NA, MSEP_i = NA)
  j <- 1
  for (i in 1:n) {
    for (deg in poly_degrees) {
      fit <- lm(mpg ~ poly(horsepower, deg, raw = TRUE), data = Auto[setdiff(1:n, i), ])
      pred <- predict(fit, newdata = Auto[i, ])
      
      MSEP_df$MSEP_i[j] <- (Auto[i, ]$mpg - pred) ^ 2
      MSEP_df$i[j] <- i
      MSEP_df$deg[j] <- deg
      
      j <- j + 1
    }    
  }
  
  MSEP_df_summary <- 
    MSEP_df %>%
    group_by(deg) %>%
    summarize(CV_n = mean(MSEP_i)) 
  
  t01 <- Sys.time()
}
t01 - t00

g <-
MSEP_df_summary %>%
  ggplot(aes(x = deg, y = CV_n)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  theme_bw() +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  xlab("Degree of Polynomial") +
  ylab("Estimated Mean Squared Error (Prediction)") +
  scale_y_continuous(breaks = seq(16, 28, 2), limits = c(16,28))

ggplotly(g)

# Compare to the hat-formula shortcut:
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1))
plot(imager::load.image("Chapter5_img01.png"), axes = FALSE) 

{
  t10 <- Sys.time()  
  n <- nrow(Auto)
  poly_degrees <- 1:10
  MSEP_df_shortcut <- tibble(deg = poly_degrees, CV_n = NA)
  for (deg in poly_degrees) {
    fit <- lm(mpg ~ poly(horsepower, deg), data = Auto)
    pred <- predict(fit)
    MSEP_df_shortcut$CV_n[deg] <- sum(  ( (Auto$mpg - pred)/(1 - hatvalues(fit)) )^2  ) / n
  }
  t11 <- Sys.time()
}
t11 - t10

all.equal(MSEP_df_summary, MSEP_df_shortcut)

# k-fold Cross Validation (Figure 5.4, right) -----

n <- nrow(Auto)
poly_degrees <- 1:10
k <- 10

# Code for assigning group numbers adapted from boot::cv.glm()
k.o <- k
kvals <- unique(round(n/(1L:floor(n/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
if (k != k.o) { warning(gettextf("'k' has been set to %f", k), domain = NA) }
f <- ceiling(n/k)
# Corresponds to the task of dividing the training set to "approximately" equal parts.

M <- 9

set.seed(1)
MSEP_k_df <- tibble(i = rep(NA, M * k * length(poly_degrees)), m = NA, deg = NA, MSEP_i = NA)
j <- 1
for (m in 1:M) {
  Auto$fold <- sample(rep(1L:k, f), n)
  # table(Auto$fold)
  for (i in 1:k) {
    for (deg in poly_degrees) {
      # for training, everything except the i-th fold:
      fit <- lm(mpg ~ poly(horsepower, deg, raw = TRUE), data = Auto %>% filter(fold != i))
      # for validation, just the i-th fold:
      df_test <- Auto %>% filter(fold == i)      
      pred <- predict(fit, newdata = df_test)
      
      MSEP_k_df$MSEP_i[j] <- mean( (df_test$mpg - pred) ^ 2 )
      MSEP_k_df$i[j] <- i
      MSEP_k_df$deg[j] <- deg
      MSEP_k_df$m[j] <- m
      
      j <- j + 1
    }    
  }
}

MSEP_k_df_summary <- 
  MSEP_k_df %>%
  group_by(deg, m) %>%
  summarize(CV_n = mean(MSEP_i)) 


ggplot(MSEP_k_df_summary) +
  geom_line(aes(x = deg, y = CV_n, color = factor(m))) +
  theme_bw() +
  scale_x_continuous(breaks = seq(2, 10, 2)) +
  xlab("Degree of Polynomial") +
  ylab("Estimated Mean Squared Error (Prediction)") +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = seq(16, 28, 2), limits = c(16,28))

# Figure 5.6, left panel -----
# Homework

# Cross-Validation on Classification Problems -----

# * Figure 5.7 -----
mixture_example <- readRDS("mixture_example.rds")
sample <- mixture_example$sample
sample_test <- mixture_example$sample_test
xp <- mixture_example$grid_probs

g0 <-
  ggplot(sample) +
  geom_point(aes(x = x1, y = x2, color = class), shape = 1) +
  scale_color_manual(values = c("BLUE", "ORANGE")) +
  theme_bw() +
  theme(legend.position = "none")

sample_01 <- sample %>% mutate(class_01 = ifelse(class == "ORANGE", 1, 0))
sample_test_01 <- sample_test %>% mutate(class_01 = ifelse(class == "ORANGE", 1, 0))

for (deg in 1:4) {
  logistic_fit <- glm(class_01 ~ poly(x1, deg) + poly(x2, deg), family = "binomial", data = sample_01)
  
  # Test error rate:
  print("deg")
  print(deg)
  print("train error rate")
  print(mean((predict(logistic_fit, type = "response") > 0.5) !=  sample_01$class_01))
  print("test error rate")
  print(mean((predict(logistic_fit, newdata = sample_test, type = "response") > 0.5) !=  sample_test_01$class_01))
  
  # Grid:
  xp_logistic <- 
    xp %>% 
    mutate(
      p_logistic_ORANGE = predict(logistic_fit, newdata = xp, type = "response"),
      classify_logistic = ifelse(p_logistic_ORANGE > 0.5, "ORANGE", "BLUE"), 
      )
  
  print(
  g0 +
    geom_point(data = xp_logistic, aes(x = x1, y = x2, color = classify_logistic), pch = ".", alpha = 0.3) +
    stat_contour(data = xp_logistic, aes(x = x1, y = x2, z = p_bayes_ORANGE), color = "purple", breaks = c(0, 0.5)) +
    stat_contour(data = xp_logistic, aes(x = x1, y = x2, z = p_logistic_ORANGE), color = "black", breaks = c(0, 0.5)) +
    xlab(element_blank()) +
    ylab(element_blank()) +
    theme(axis.ticks = element_blank(), axis.text.y = element_blank(), axis.text.x = element_blank())
  )
}

# Figure 5.8, left -----

# Test/train error rates:
mixture_example <- readRDS("mixture_example.rds")
sample <- mixture_example$sample
sample_test <- mixture_example$sample_test
xp <- mixture_example$grid_probs

sample_01 <- sample %>% mutate(class_01 = ifelse(class == "ORANGE", 1, 0))
sample_test_01 <- sample_test %>% mutate(class_01 = ifelse(class == "ORANGE", 1, 0))

deg_seq <- 1:10
error_rates <- tibble(deg = deg_seq, test = NA, train = NA)
for (deg in deg_seq) {
  logistic_fit <- 
    glm(class_01 ~ poly(x1, deg, raw = TRUE) + poly(x2, deg, raw = TRUE), family = "binomial", data = sample_01)
  
  error_rates$train[deg] <- mean((predict(logistic_fit, type = "response") > 0.5) !=  sample_01$class_01)
  error_rates$test[deg] <- mean((predict(logistic_fit, newdata = sample_test, type = "response") > 0.5) !=  sample_test_01$class_01)
}

# k-fold CV:
n_train <- nrow(sample_01)
k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
if (k != k.o) { warning(gettextf("'k' has been set to %f", k), domain = NA) }
f <- ceiling(n_train/k)

ERR_k_fold <- tibble(i = rep(NA, k * length(deg_seq)), deg = NA, ERR_i = NA)
j <- 1
sample_01$fold <- sample(rep(1L:k, f), n_train)
# table(sample_01$fold)
for (fld in 1:k) {
  current_train_folds <- sample_01 %>% filter(fold != fld)
  current_test_fold <- sample_01 %>% filter(fold == fld)
  
  for (deg in deg_seq) {
    logistic_fit <- 
      glm(
        class_01 ~ poly(x1, deg, raw = TRUE) + poly(x2, deg, raw = TRUE), 
        family = "binomial", 
        data = current_train_folds
      )
    
    ERR_k_fold$ERR_i[j] <- 
      mean((predict(logistic_fit, newdata = current_test_fold, type = "response") > 0.5) !=  current_test_fold$class_01)
    
    ERR_k_fold$i[j] <- fld
    ERR_k_fold$deg[j] <- deg
    
    j <- j + 1
  }    
}

ERR_k_fold_summary <- 
  ERR_k_fold %>%
  group_by(deg) %>%
  summarize(k_fold = mean(ERR_i)) 

ERR_compare <-
  error_rates %>%
  left_join(ERR_k_fold_summary, by = "deg") %>%
  pivot_longer(cols = -deg, names_to = "Type", values_to = "Error rate")

ggplot(ERR_compare) +
  geom_line(aes(x = deg, y = `Error rate`, color = Type)) +
  theme_bw() +
  xlab("Order of polynomials used")

# The Bootstrap -----

# Step 1: (unrealistic sceanrio where we can sample from the distribution of the 
# underlying random variables)
true_Sigma <-
  matrix(
    c(
      1, 0.5,
      0.5, 1.25
    ),
    nrow = 2
  )

given_sample <- ISLR2::Portfolio

par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1))
plot(imager::load.image("Chapter5_img02.png"), axes = FALSE) 

set.seed(1)
M <- 1000
n <- 100
est_alpha_vec <- numeric(M)
for (m in 1:M) {
  sample <- mvtnorm::rmvnorm(n, sigma = true_Sigma)
  est_cov_mat <- cov(sample)
  est_alpha_vec[m] <- 
    (est_cov_mat[2, 2] - est_cov_mat[1, 2]) / (est_cov_mat[1, 1] + est_cov_mat[2, 2] - 2 * est_cov_mat[1, 2])
}

range(est_alpha_vec)
(est_alpha <- mean(est_alpha_vec))

# Compare to true alpha:
(true_alpha <- 
    (true_Sigma[2, 2] - true_Sigma[1, 2]) / 
    (true_Sigma[1, 1] + true_Sigma[2, 2] - 2 * true_Sigma[1, 2])
    )

(sd_est_alpha <- sd(est_alpha_vec))

dev.off()
ggplot(tibble(est_alpha = est_alpha_vec)) +
  geom_histogram(aes(x = est_alpha, y = after_stat(density)), bins = 15, color = "black", fill = "orange") +
  theme_bw() +
  xlim(c(0.3, 0.9))

# Step 2:
# Actually using the bootstrap method to sample with replacement from the original
# data set to create multiple ootstrap data sets, and use these to estimate the distribution
# of the statistic of interest!
B <- 1000
alpha_bstp_vec <- numeric(B)
for (b in 1:B) {
  bootstrap_sample_idx <- sample(n, replace = TRUE)
  Z <- given_sample[bootstrap_sample_idx, , drop = F]  # create bootstrap data set
  est_cov_mat <- cov(Z)
  alpha_bstp_vec[b] <-
    (est_cov_mat[2, 2] - est_cov_mat[1, 2]) / (est_cov_mat[1, 1] + est_cov_mat[2, 2] - 2 * est_cov_mat[1, 2])
}

sd(alpha_bstp_vec)

(g1 <-
  ggplot(tibble(est_alpha = alpha_bstp_vec)) +
  geom_histogram(aes(x = est_alpha, y = after_stat(density)), bins = 15, color = "black", fill = "lightblue") +
  theme_bw() +
  xlim(c(0.3, 0.9)))

# Bootstrap CI:
conf <- 0.05
# With 95% probability, alpha is in 
(bootstrap_CI <- quantile(alpha_bstp_vec, c(conf/2, 1 - conf/2)))

g1 + geom_vline(xintercept = bootstrap_CI, col = "blue")

