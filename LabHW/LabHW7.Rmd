---
title: "LabHW7"
author: "Jerry Obour"
date: "2025-04-15"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, warning=FALSE}

library(dplyr)
library(tidyverse)
library(ggplot2)
library(plotly)
library(leaps)
library(glmnet)

```



## Question 1

```{r generate-data}
set.seed(1)

n <- 1000
p <- 20
X <- matrix(rnorm(n*p), nrow = n, ncol = p) 
b0 <-  4.8
beta <- c(0, -3.1, 0, 1.6, 0, 0, -2.3, 0, 4.2, 0, 0, 0, 2.1, 0, 0, 3.3, 0, 0, 0, 0)
e <- rnorm(n, sd=0.5)
Y <- b0 + X %*% beta + e
df <- data.frame(Y = Y, X)

```

```{r split-data}

train_indices <- sample(1:nrow(df), size = 0.1 * NROW(df), replace = FALSE)
df_train <- df[train_indices, ]
df_test <- df[-train_indices, ]

```


```{r}

nvmax <- p
best_sub <- regsubsets(Y ~ ., data = df_train, nbest = 1, really.big = TRUE, nvmax = nvmax)

fit_full <- lm(Y~., data=df_train)

best_fit <- summary(best_sub)


summary(fit_full)



train_mse <- numeric(nvmax)
test_mse <- numeric(nvmax)

for (i in 1:nvmax) {
  # Identifying predictors included in the model
  predictors <- names(coef(best_sub, id = i))
  
  model_formula <- as.formula(paste("Y ~", paste(predictors[-1], collapse = " + ")))
  model_train <- lm(model_formula, data = df_train)
  
  train_pred <- predict(model_train, newdata = df_train)
  test_pred <- predict(model_train, newdata = df_test)
  
  train_mse[i] <- mean((df_train$Y - train_pred)^2)
  test_mse[i] <- mean((df_test$Y - test_pred)^2)
}

mse_data <- data.frame(ModelSize = 1:nvmax, TrainMSE = train_mse, TestMSE = test_mse)
mse_data_long <- tidyr::pivot_longer(
  mse_data,
  cols = c("TrainMSE", "TestMSE"),
  names_to = "Set",
  values_to = "MSE"
) %>%
  mutate(Set = recode(Set,
                      TrainMSE = "Train",
                      TestMSE = "Test"))



```


```{r plots}

ggplot(mse_data_long, aes(x = ModelSize, y = MSE, color = Set)) +
  geom_line(size = 0.8) +
  geom_point(size = 1.5) +
  labs(
    title = "MSE vs. Model Size",
    x = "Model Size",
    y = "Mean Squared Error",
    color = "Dataset"
  ) +
  scale_color_manual(values = c("Train" = "blue", "Test" = "red")) +
  theme_minimal()

```

```{r least-trainMSE}
mse_data %>%
  filter(TrainMSE==min(TrainMSE))

```

```{r least-testMSE}
mse_data %>%
  filter(TestMSE==min(TestMSE))

```
```{r}
(model_vars_6 <- coef(best_sub,id=6))
```

From the results above, we see that the training MSE decreases continuously as the number of predictors is increased(thus more predictors are added to the model) and this leads  to overfitting with increasing number of predictors. And so we record that the model with the least training MSE is the model involving all predictors.
On the other hand, we found that using the test data, the test MSE chooses the model with 6 predictors. The test MSE decreases to the right model and as we increase the number of predictors, there is no further improvement in the test MSE but rather increases when the model overfits or underfits.
Moreover, the true model is of the form $Y = 4.8 -3.1X_2+1.6X_4-2.3X_7+4.2X_9+2.1X_{13}+3.3X_{16}$ whilst the best model chosen is of the form $\hat{Y} = 4.816970 -3.076373X_2+1.600726X_4-2.322133X_7+4.081450X_9+1.960029X_{13}+3.315348_{16} $ and has the least test MSE. The coefficients are pretty much close to each other.

```{r distance-plot,warning=FALSE}
beta_distance <- numeric(p)

# Calculate distance for each value of r
for (j in 1:p) {
  coef_j <- coef(best_sub, id = j)
  beta_distance[j] <- sqrt(sum((coef(fit_full)[-1] - coef_j[-1])^2))
}


beta_df <- data.frame(
  Predictors = 1:p,
  Distance = beta_distance
)

ggplot(beta_df, aes(x = Predictors, y = beta_distance)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 1.5) +
  labs(
    title = "Distance from Full Model Coefficients vs. Number of Predictors (j)",
    x = "Number of Predictors (j)",
    y = "Distance from Full Model Coefficients"
  ) +
  theme_minimal()

```


In data science terms, this plot measures the Euclidean distance between the coefficient vector of the best $j$
r-variable model and that of the full model. It's a way to quantify how different the simplified models are from the full model in terms of parameter estimates. This metric is often referred to as parameter instability or coefficient shrinkage (in a non-penalized context). A small distance indicates that the best model of size $j$ is very similar to the full model in terms of which variables it includes and how much influence they have. The distance will be zero when the best model includes all the predictors $j=p$, as it's identical to the full model.
The test MSE plot from part (d) shows how well models of different sizes generalize to unseen data.
Comparing the results here to the results in (d), the Euclidean distance plot focuses on coefficient stability, that is how much the model coefficients deviate from the full model as variables are removed. It's possible for a model to have low test MSE but a large distance from the full model — this means the model is good for prediction, but structurally different. Conversely, a model could be close in terms of coefficients but perform worse due to overfitting.

## Question 2

```{r splitting-data}

set.seed(1)
Credit <- ISLR2::Credit
n <- nrow(Credit)
x <- scale(model.matrix(Balance ~ ., data = Credit)[ , -1])
y <- Credit$Balance

set.seed(1)
n_train <- 0.8 * n
n_test <- n - n_train
train <- rep(FALSE, n)
train[sample(1:n, n_train, replace = FALSE)] <- TRUE
test <- (!train)

x_train <- x[train, ]
y_train <- y[train]
x_test <- x[test, ]
y_test <- y[test]

scaled_credit_df_train <- bind_cols(Balance = y_train, x_train)
scaled_credit_df_test <- bind_cols(Balance = y_test, x_test)

```


```{r perform-loocv}

lambda_grid <- 10 ^ seq(10, -2, length = 100)
ridge_reg <- cv.glmnet(x_train, y_train, lambda = lambda_grid, nfolds = ncol(x_train),alpha=0) 

```



```{r loocv-vs-ols}
# since the intercepts are not predictors, we exclude them
(coeff_ridge <- coef(ridge_reg,s="lambda.min")[-1])

ols_model <- lm(Balance ~ .,data = scaled_credit_df_train)
(coeff_ols <- coef(ols_model)[-1])

coef_df <- data.frame(
  Predictor = names(coeff_ols),
  Ridge = coeff_ridge,
  OLS = coeff_ols
)


```


```{r}

coef_df <- data.frame(
  Predictor = names(coeff_ols),
  Ridge = coeff_ridge,
  OLS = coeff_ols
)

# Scatter plot: Ridge Coefficients vs OLS Coefficients
ggplot(coef_df, aes(x = Ridge, y = OLS, color = Predictor)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + 
  labs(
    title = "Scatter Plot: OLS vs Ridge Coefficients",
    x = "Ridge Coefficients",
    y = "OLS Coefficients",
    color = "Predictor"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = "none"
  )

```

Clearly, the scatter plot shows a strong linear relationship between the ridge coefficients and the OLS coefficients which indicates that Ridge regression shrinks the OLS estimates toward zero. Larger coefficients experience more shrinkage, while smaller coefficients are relatively unaffected. This is consistent with Ridge's regularization behavior, which stabilizes estimates when the model is overfit or the is colinearity among predictors.


```{r bootstrap-params}

N <- nrow(x_test)
M <- 1000

```

```{r bootstrap-SE}

lambda_v <- ridge_reg$lambda.min

coeff_matrix <- matrix(NA, nrow = M, ncol = ncol(x_train) + 1)
for (i in 1:M){
  boot_indices <- sample(1:N, size = N, replace = TRUE)
  x_boot <- x_train[boot_indices, ]
  y_boot <- y_train[boot_indices]
  
  fit <- glmnet(x_boot, y_boot, alpha = 0, lambda = lambda_v, standardize = TRUE)
  coeffs <- as.numeric(coef(fit))  
  coeff_matrix[i, ] <- coeffs
  }

ridge_se <- apply(coeff_matrix, 2, sd)



coeff_matrixOLS <- matrix(NA, nrow = M, ncol = ncol(x_train) + 1)
for (i in 1:M){
  boot_indices <- sample(1:N, N, replace = TRUE)
  m_Fit <- lm(Balance ~ ., data = scaled_credit_df_train[boot_indices, ])
  
  # Store the coefficients (including intercept)
  coeff_matrixOLS[i, ] <- as.numeric(m_Fit$coefficients)
}
ols_se <- apply(coeff_matrixOLS, 2, sd)


```

```{r}

SE_df <- data.frame(
  Ridge = ridge_se,
  OLS = ols_se
)

# Scatter plot: Ridge Coefficients vs OLS Coefficients
ggplot(SE_df, aes(x = Ridge, y = OLS)) +
  geom_point(size = 1.5) +
  labs(
    title = "Standard Error of OLS vs Ridge",
    x = "Ridge SE",
    y = "OLS SE",
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

```


Similarly, we see clearly an almost linear relationship between the two data. Ridge regression reduces the variance of coefficient estimates, especially for variables having large standard errors under OLS. This is expected because Ridge introduces bias to reduce variance the bias variance trade-off, which makes the estimates more stable across bootstrap samples. This effect is most noticeable for coefficients that are less stable under OLS.

```{r}
(ridge_se)
(ols_se)

```


```{r bootstrap-MSE}

ridge_MSE <- matrix(NA, nrow = M, ncol = 1)
for (i in 1:M) {
  
  boot_indices <- sample(1:N, N, replace = TRUE) 
  x_boot <- x_test[boot_indices, ]
  y_boot <- y_test[boot_indices]
  
  
  ridge_predictions_direct <- predict(ridge_reg, newx = x_boot, s = "lambda.min")
  ridge_MSE[i, ] <- mean((y_boot - ridge_predictions_direct)^2)
}
(ridge_CI <- quantile(ridge_MSE, c(0.025, 0.975)))




# Initialize a matrix to store MSE values for OLS regression
ols_MSE <- matrix(NA, nrow = M, ncol = 1)
for (i in 1:M) {
  boot_indices <- sample(1:N, N, replace = TRUE)  
  x_boot <- scaled_credit_df_test[boot_indices, ]  
  y_boot <- y_test[boot_indices]
  
  ols_predictions_direct <- predict(ols_model, newdata = as.data.frame(x_boot))
  
  ols_MSE[i, ] <- mean((y_boot - ols_predictions_direct)^2)
}

(ols_CI <- quantile(ols_MSE, c(0.025, 0.975)))

```

Using Bootstrap method, we see that both Ridge and OLS have very similar 95% bootstrap confidence intervals for prediction error, with overlapping ranges and nearly identical lower bounds. This suggests that their predictive performances are statistically comparable in this setting. The slightly wider interval for Ridge could indicate more variability in some bootstrap samples, possibly due to the tradeoff between bias and variance introduced by regularization. Overall, the difference is not substantial, and no method clearly outperforms the other based solely on the confidence intervals.


```{r kfold-ridge}

ridgekf_reg <- cv.glmnet(x_train, y_train, lambda = lambda_grid, nfolds = 10,alpha=0)

(coeff_ridgekf <- coef(ridgekf_reg,s="lambda.min")[-1])

coefkf_df <- data.frame(
  Predictor = names(coeff_ols),
  Ridge = coeff_ridgekf,
  OLS = coeff_ols
)

# Scatter plot: Ridge Coefficients vs OLS Coefficients
ggplot(coefkf_df, aes(x = Ridge, y = OLS, color = Predictor)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Scatter Plot: OLS vs k-fold Ridge Coefficients",
    x = "k-fold Ridge Coefficients",
    y = "OLS Coefficients",
    color = "Predictor"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = "none"
  )

```


Here, we similarly observe a linear relationship between the two coefficients indicating that ridge regression with 10-fold CV shrinks the OLS coefficients toward zero, especially those with larger magnitudes. We also notice that the shrinkage is proportional to the size of the OLS coefficients — bigger coefficients are pulled back more.The results obtained for the ridge regression are more stabled compared to the OLS.

```{r}

lambda_vkf <- ridgekf_reg$lambda.min

coeff_matrixkf <- matrix(NA, nrow = M, ncol = ncol(x_train) + 1)
for (i in 1:M){
  boot_indices <- sample(1:N, size = N, replace = TRUE)
  x_boot <- x_train[boot_indices, ]
  y_boot <- y_train[boot_indices]
  
  fit <- glmnet(x_boot, y_boot, alpha = 0, lambda = lambda_vkf, standardize = TRUE)
  coeffs <- as.numeric(coef(fit))  
  coeff_matrixkf[i, ] <- coeffs
}

(ridge_sekf <- apply(coeff_matrixkf, 2, sd))


SEkf_df <- data.frame(
  Ridge = ridge_sekf,
  OLS = ols_se
)

# Scatter plot: Ridge Coefficients vs OLS Coefficients
ggplot(SEkf_df, aes(x = Ridge, y = OLS)) +
  geom_point(size = 1.5,color='red') +
  labs(
    title = "Standard Error of OLS vs k-fold Ridge",
    x = " 10-fold Ridge SE",
    y = "OLS SE",
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = "none"  
  )


```

The scatter plot of bootstrapped standard errors from Ridge (10-fold CV) vs OLS will show that Ridge consistently produces smaller or similar standard errors compared to OLS, particularly for coefficients with large variability in OLS. This illustrates Ridge regression's strength in stabilizing coefficient estimates by trading a bit of bias for a notable reduction in variance. In data science terms, Ridge improves the reliability and ability to generalize the coefficient estimates, especially when working with noisy or collinear predictors

```{r}

ridgekf_MSE <- matrix(NA, nrow = M, ncol = 1)
for (i in 1:M) {
  
  boot_indices <- sample(1:N, N, replace = TRUE) 
  x_boot <- x_test[boot_indices, ]
  y_boot <- y_test[boot_indices]
  
  
  ridge_predictions_direct <- predict(ridgekf_reg, newx = x_boot, s = "lambda.min")
  ridgekf_MSE[i, ] <- mean((y_boot - ridge_predictions_direct)^2)
}
(ridgekf_CI <- quantile(ridgekf_MSE, c(0.025, 0.975)))

```

The 95% bootstrap confidence intervals for test error show that Ridge regression with 10-fold CV offers slightly more stable performance across resampled datasets with replacement compared to OLS $(7341.281, 14526.469)$, as indicated by its slightly narrower interval and lower upper bound. However, the substantial overlap between the two intervals means that the predictive performance difference is not statistically significant. Ridge's advantage lies more in model stability and coefficient shrinkage.

f) Confidence intervals derived using bootstrapping would not effectively represent the true sampling distribution of the coefficients because ridge regression introduces bias by design.  Standard bootstrap-based Confidence intervals can be misinterpreted in the traditional frequentist sense because of the regularization, which modifies the distribution.

## Question 3
This section is left out for submission later because my group had trouble with data cleaning. We spoke to Prof. Shai who has granted an extension. Thanks!