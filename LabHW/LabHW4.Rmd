---
title: "Untitled"
author: "Jerry Obour"
date: "2025-03-01"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(ggplot2)
library(plotly)
```

# Question 1:

```{r}

set.seed(1)      # set a random seed for reproducibility
n <- 100         # set a sample size

beta0 <- 0.2
beta1 <- 0.5
beta2 <- -0.5

x1 <- runif(n, -5, 5)
x2 <- runif(n, -10, 10)

lin_term <- beta0 + beta1 * x1 + beta2 * x2
p <- exp(lin_term) / ( 1 + exp(lin_term) )

y <- rbinom(n, 1, p)

```

a) In the above lines of code, we build a logistic model with two predictors $\{x_1,x_2\}$. Beyond the seed and sample size initializing, the code begins by defining the logistic regression coefficients or model parameters and assigning them values. It then generates 100 random uniform samples for the two predictors, with the first predictor having its values in the range $[-5,5]$ and the second predictor having its values in the range $[-10,10]$ respectively. It then defines the model line, which is more like the log of the model given by $line\_term = \beta_0 + \beta_1x_1 + \beta_2x_2$ . Finally, the actual logistic regression model is defined $y=\dfrac{\exp(line\_term)}{1+\exp(line\_term)}$.

b) The plots shown are marginal plots for the two predictor variables $x_1$ and $x_2$ against the response variable $y$:
** In the first plot, we having the marginal distribution of the data for the predictor variable $x_1$ against the response variable $y$. It is seen that majority of the data are classified as $0$ when $x_1$ is in the range $(-5,-1)$ and only a few are classified as $1$. Moreover, when $x_1$ is in the range $(-1,5)$ majority of the data are classified as $1$ and only of few of the responses are classified as $0$.
** In the second plot, we have the marginal distribution of the data for the predictor variable $x_2$ against the response variable $y$. In contrast, when $x_2$ is in the range $(-10,0)$, majority of the responses are classified as $1$ and when $x_2$ is in the range $(0,10)$, majority of the responses are classified as $1$.

c) 
```{r}
data_set <- tibble(y=y,x1=x1,x2=x2)
head(data_set)
```


```{r}
model <- glm(y~x1+x2, family="binomial", data=data_set)
summary(model)
```
```{r}

prediction_prob <- predict(model, newdata = data_set, type = "response")
p_classes <- ifelse(prediction_prob > 0.5, 1, 0)

data_set <-
  data_set %>%
  mutate(yhat = p_classes)
```

```{r}

wrong_class <-
  data_set %>%
  filter(y != yhat) %>%
  summarise(n=n())

(wrong_class/100)

```

From the above results, the estimated coefficients are slightly different from the true coefficients of the mode. When compared, whilst $\beta_0 = 0.2$, $\hat{\beta_0} = 0.2746$ with a standard error of $0.3475$. Also, whilst $\beta_1 = 0.5$, $\hat{\beta_1} =0.6572$ with a standard error of $0.1786$ and finally, whilst $\beta_2 = -0.5$, $\hat{\beta_2} = -0.5821$ with a standard error of $0.1179$. Comparing the results, we can see that the estimated coefficients are not exactly the same as the true parameters but are somewhat close.\
Now when the threshold for classifying an observation as $1$ is $0.5$, we estimate the error by counting the number of predictions which are not properly classified and dividing the results of the count by $100$ which is the total number of predictions. The results above indicate that the error rate is $0.1$, thus there were a total of $10$ mis-classifications.


# QUestion 2
a)
$$\hat{P} (Y=orange | X=x) = \dfrac{e^{ \hat{\beta_0} + \hat{\beta_1}x }}{ 1 + e^{ \hat{\beta_0} + \hat{\beta_1}x}} $$\

$$ \hat{P} (Y=apple | X=x) = 1-\hat{P} (Y=orange | X=x) $$

$$\hat{P} (Y=apple | X=x) = \dfrac{1}{ 1 + e^{ \hat{\beta_0} + \hat{\beta_1}x}} $$

Estimating the log odds of orange versus apple model

$$\log~\left( \dfrac{\hat{P} (Y=orange | X=x)}{\hat{P} (Y=apple | X=x)} \right) =\log\left(  \dfrac{e^{ \hat{\beta_0} + \hat{\beta_1}x }}{ 1}\right)$$

$$\log~\left( \dfrac{\hat{P} (Y=orange | X=x)}{\hat{P} (Y=apple | X=x)} \right) = \hat{\beta_0} + \hat{\beta_1}x $$\


b)
$$\hat{P} (Y=orange | X=x) = \dfrac{e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }} {e^{ \hat{\alpha}_{apple_0} + \hat{\alpha}_{apple_1}x} + e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }}  $$
$$ \hat{P} (Y=apple | X=x) = \dfrac{e^{ \hat{\alpha}_{apple_0} + \hat{\alpha}_{apple_1}x }} {e^{ \hat{\alpha}_{apple_0} + \hat{\alpha}_{apple_1}x} + e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }} $$
Estimating the log odds of orange versus apple model
$$\log~\left( \dfrac{\hat{P} (Y=orange | X=x)}{\hat{P} (Y=apple | X=x)} \right) =\log\left(  \dfrac{e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }}{ e^{ \hat{\alpha}_{apple_0} + \hat{\alpha}_{apple_1}x } }\right)$$
$$\log~\left( \dfrac{\hat{P} (Y=orange | X=x)}{\hat{P} (Y=apple | X=x)} \right) = ( \hat{\alpha}_{orange_0} - \hat{\alpha}_{apple_0} ) + ( \hat{\alpha}_{orange_1} - \hat{\alpha}_{apple_1} )x $$\

c) 
By comparing results from $(a)$ and $(b)$, we can draw the following conclusions:
$$ \hat{\beta_0} \equiv \hat{\alpha}_{orange_0} - \hat{\alpha}_{apple_0} \quad \text{and}$$
$$\hat{\beta_1} \equiv \hat{\alpha}_{orange_1} - \hat{\alpha}_{apple_1}$$\

With $\hat{\beta_0} = 2$  and $\hat{\beta_1}=-1$,\

We have $$ \hat{\alpha}_{orange_0} - \hat{\alpha}_{apple_0} = 2 $$\

and $$ \hat{\alpha}_{orange_1} - \hat{\alpha}_{apple_1} = -1 $$\

At this point, we cannot further estimate the precise values for $\hat{\alpha}_{orange_0},\hat{\alpha}_{orange_1}, \hat{\alpha}_{apple_0}$ and $\hat{\alpha}_{apple_1}$. But when we rewrite the supposed model from my friend,
thus 

$$\hat{P} (Y=orange | X=x) = \dfrac{e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }} {e^{ \hat{\alpha}_{apple_0} + \hat{\alpha}_{apple_1}x} + e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }}  $$

$$ \hat{P} (Y=orange | X=x) = \dfrac{1}{e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x}} \times \dfrac{e^{ \hat{\alpha}_{orange_0} + \hat{\alpha}_{orange_1}x }} {e^{ (\hat{\alpha}_{apple_0} -\hat{\alpha}_{orange_0}) + (\hat{\alpha}_{apple_1}-\hat{\alpha}_{orange_1} )x} + 1}  $$

$$ \hat{P} (Y=orange | X=x) = \dfrac{1} {e^{ (\hat{\alpha}_{apple_0} -\hat{\alpha}_{orange_0}) + (\hat{\alpha}_{apple_1}-\hat{\alpha}_{orange_1} )x} + 1}  $$

$$ \hat{P} (Y=orange | X=x) = \dfrac{1} {e^{ -(\hat{\alpha}_{orange_0} - \hat{\alpha}_{apple_0} ) - (\hat{\alpha}_{orange_1} - \hat{\alpha}_{apple_1} )x} + 1}  $$
But $$ \hat{\alpha}_{orange_1} - \hat{\alpha}_{apple_1}  \quad \text{and} \quad \hat{\alpha}_{orange_0} - \hat{\alpha}_{apple_0} $$
are given
$$\implies \hat{P} (Y=orange | X=x) = \dfrac{1} {e^{ - 2 +x} + 1} $$
And thus the above will be similar to my model when rewritten. Thus by comparison, we are able to estimate the coefficients of my friend's model and rewrite such that it would look like mine. Note that specific values of $\hat{\alpha}$ cannot be estimated since there are 2 equations and 4 variables.


d) $$\hat{\alpha}_{orange_0} = 1.2, \quad \hat{\alpha}_{orange_1} = -2, \quad \hat{\alpha}_{apple_0} = 3 \quad \text{and } \hat{\alpha}_{apple_1} =0.6$$
$$\implies \hat{\beta}_0 = 1.2-3=-1.8 \quad \text{ and } \hat{\beta}_1 = -2-0.6 = -2.6$$
It is simpler in this section since we can from the values of $\hat{\alpha}$ estimate the values of $\hat{\beta}$ by using the equations we derived from comparing the coefficients.
Whilst my model will be 
$$\hat{P} (Y=orange | X=x) = \dfrac{e^{ -1.8 -2.6 x }}{ 1 + e^{ -1.8 - 2.6x}} $$\
My friend's model will now be 
$$\hat{P} (Y=orange | X=x) = \dfrac{e^{ 1.2 -2x }} {e^{ 3 + 0.6x} + e^{ 1.2-2x }}  $$

d) Applying both models to a data set with $2,000$ observations, it is expected that the predicted values are equal for the two parameterizations at all times. This is seen in the two models by simplifying both models or even using their log-odd ratios and comparing their parameters, both models tend to achieve same weights that are equivalent. And for a sample of such size, it is expected that the difference in both models will be minimal. Moreover, theoretically the softmax regression is a generalization of the logistic regression for classes that could be 2 or more whilst the logistic regression only works on problems with two classes. 

# Question 3

```{r}
library(ISLR2)
```
Loading the data set Auto:
```{r}
data_auto <- ISLR2::Auto
```

```{r}
head(data_auto)
```

We perform binary encoding by using the `mutate()` function within which we use `ifelse()` to check for records where the `mpg` value is greater than the median of all `mpg` values. If this is True, then we assign `1` to its corresponding `mpg01` index and `0` otherwise. The mutate function helps in creating the extra column `mpg01`.
```{r}
data_auto <- 
  data_auto %>%
  mutate(mpg01 = ifelse(mpg>median(mpg),1,0))
```

```{r}
data_view <-
  data_auto %>%
  select(mpg,mpg01)

NROW(data_view)
```


b)
```{r}
pairs(data_auto[,c("mpg01","cylinders","displacement","horsepower","weight","acceleration","year","origin","name")])

```


```{r}
plot(data_auto$cylinders,data_auto$mpg01,xlab = "Cylinders",ylab = "mpg01")
```
```{r}
plot(data_auto$year,data_auto$mpg01,xlab = "Year",ylab = "mpg01")
```


```{r}
plot(data_auto$origin,data_auto$mpg01, xlab = "Origin",ylab = "mpg01")
```

```{r}
plot(data_auto$name,data_auto$mpg01, xlab = "Name",ylab = "mpg01")
```
```{r}

par(mfrow=c(2,4))
boxplot(cylinders ~ mpg01, data = data_auto, main = "Cylinders vs mpg01")
boxplot(displacement ~ mpg01, data = data_auto, main = "Displacement vs mpg01")
boxplot(horsepower ~ mpg01, data = data_auto, main = "Horsepower vs mpg01")
boxplot(weight ~ mpg01, data = data_auto, main = "Weight vs mpg01")
boxplot(acceleration ~ mpg01, data = data_auto, main = "Acceleration vs mpg01")
boxplot(year ~ mpg01, data = data_auto, main = "Year vs mpg01")
boxplot(origin ~ mpg01, data = data_auto, main = "Origin vs mpg01")
boxplot(name ~ mpg01, data = data_auto, main = "Name vs mpg01")

```

From the above correlation plots, the focus is on the first row which is responsible for describing the relationship between `mgp01` as a response variable and the other variables as predictors. Clearly, since `mpg` has two classes and for each predictor there are 392 records, it can be seen from the above scatter plot that there is a relationship some kind of good association between `mpg01` and the predictors `displacement, horsepower, weight, acceleration`. When the plots between `mpg01` and the predictors `cylinders, name, origin, year` are zoomed in carefully, it can be seen that there is not good enough association between the response variable and these predictors except for `cylinders` which seems a little different. Following from the Boxplot, it is seen that there is a strong relationship between `mpg01` and the predictors `cylinders, displacement, horsepowe, weight` and weak relationship between `mpg01` and the predictors `acceleration, year, origin, name`. It suffices to conclude that the predictors to be picked as good choices are `cylinders, displacement, horsepower, weight`


c)
```{r}
train_indices <- sample(1:NROW(data_auto), size = 0.7 * NROW(data_auto), replace = FALSE)

train_data <- data_auto[train_indices, ]
test_data <- data_auto[-train_indices, ]
```
In the above we use the sample function to randomly select $70%$ of the total indices that make up the total number of records from the data. Specifying `replace=FALSE` ensures that no index is selected more than once. We then set the selected indices as our `train_indices`. Now we filter the selected indices from the entire data by selecting rows that conform to the indices and we assign the results the variable `train_data`. The rest of the data is assigned the name `test_data`.\


d)\
To perform logistic regression test, we first select the important columns for the test. In so doing, I used the r select function to select from the train dataset the columns which are to be used in the model. Moreover, I do well to separate the response column for the test data from the test predictors for easy workflow. In all three instances, I use the `select()` function.
```{r}
train_data <-
  train_data %>%
  select(mpg01,cylinders,displacement,horsepower,weight)

test_y <- 
  test_data %>%
  select(mpg01)

test_x <- 
  test_data %>%
  select(cylinders,displacement,horsepower,weight)

```

Using the `glm` function, I fit the model `logreg_model` by using the training dataset from the chunk above.And below is a summary of the model including the coefficients, their standard errors and p-values.
```{r}
logreg_model <- glm(mpg01 ~ ., family = "binomial", data=train_data)
summary(logreg_model)
```

In the chunk below, we make the prediction on the test dataset which returns probability values.
```{r}

logreg_predict <- predict(logreg_model,newdata = test_x, type = "response")

```

I create a tibble to contain both the true response for the test as well as the probability values predicted
```{r}
result_data <- tibble(true_y = test_y$mpg01)
result_data$pred_prob <- logreg_predict
```

Using the mutate function with ifelse statement, I set the threshold to $0.5$ and set all probabilities above the threshold to `1` and any other recorded probability in the prediction to `0`. I record the encoded prediction in a new column labeling it as `pred_y`.\
The test error is calculated by using the `summarise()` function and in it, we take the mean of all false predictions, thus by comparing the `true_y` to `pred_y` and counting the instances where the results do not much.With the `mean()` function, we estimate the proportion of such incorrect predictions to the total predictions. The test error is as seen below:
```{r}
result_data <-
  result_data %>%
  mutate(pred_y=ifelse(pred_prob>0.5,1,0))

result_data %>%
  summarise(test_error = mean(true_y != pred_y))
```


e)\
In this section, I further split the train data into its `X` and `Y` components corresponding to predictors and response data for the training.
```{r}
head(train_data)
```

```{r}
train_y <- 
  train_data %>%
  select(mpg01)

train_x <-
  train_data %>%
  select(cylinders,displacement,horsepower,weight)

```

```{r}
library(class)
```

Using the `knn` function in R, I conduct a test using $k=1$ to see the outcome:
```{r}
pred_k1 <- knn(
  train = train_x,
  test = test_x,
  cl = train_y$mpg01,
  k=1
)

```

```{r}
#(pred_k1)
```

Following the same procedure in `d)` I estimate the value for the test error.
```{r}
result_knn <- tibble(true_y = test_y$mpg01)
result_knn$pred_y <- pred_k1

result_knn %>%
  summarise(test_error = mean(true_y != pred_y))
```

Generalizing for several values of $k$, I set up a sequence of values from 5 to 100 with a constant difference of 5 as my $k$ values. I combine the chunks of codes above in this section of the problem in a loop. During each iteration, I train and test the model by picking a $k$ value from the sequence $N$ on both the train and test datasets. I then estimate the test error for the prediction by using the test_y from the test set.
```{r}
N=seq(1,100)
knn_test <- tibble(kvalue=integer(length(N)), test_error=numeric(length(N)))

for (k in 1:length(N)){
  
  pred_k <- knn(
  train = train_x,
  test = test_x,
  cl = train_y$mpg01,
  k=N[k]
    )
  
  result_knn <- tibble(true_y = test_y$mpg01)
  result_knn$pred_y <- pred_k
  
  test_ER <- 
    result_knn %>%
    summarise(test_error = mean(true_y != pred_y))
  
  knn_test$kvalue[k] = N[k]
  knn_test$test_error[k] = test_ER$test_error
  
}

```

Below are the estimated test errors for each of the values of $k$ used in the model.
```{r}
#knn_test$kvalue
```

The small table below indicates the value(s) of $k$ which yield the minimum test error:
```{r}
knn_test %>%
  filter(test_error == min(test_error))

```

```{r}
min_test_K <-
  knn_test %>%
  filter(test_error == min(test_error))


ggplot(knn_test, aes(x=kvalue, y=test_error)) +
  geom_line()+
  geom_point()+
  geom_point(data=min_test_K, aes(x=kvalue,y=test_error), color='red', size=3, shape='triangle')

```